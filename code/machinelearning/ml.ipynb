{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11ee9955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 correct label\n",
      "7 network's answer\n",
      "2 correct label\n",
      "2 network's answer\n",
      "1 correct label\n",
      "1 network's answer\n",
      "0 correct label\n",
      "0 network's answer\n",
      "4 correct label\n",
      "4 network's answer\n",
      "1 correct label\n",
      "1 network's answer\n",
      "4 correct label\n",
      "9 network's answer\n",
      "9 correct label\n",
      "4 network's answer\n",
      "5 correct label\n",
      "1 network's answer\n",
      "9 correct label\n",
      "9 network's answer\n",
      "performance =  0.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "class neuralNetwork:\n",
    "    def __init__(self , inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        self.lr = learningrate\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        self.activation_function = lambda x:scipy.special.expit(x)\n",
    "        pass\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        output_errors = targets - final_outputs\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "        pass\n",
    "    def query(self, inputs_list):\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "input_nodes = 28*28\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "learning_rate = 0.3\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes,learning_rate)\n",
    "training_data_file = open(\"mldata/mnist_train_100.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    for record in training_data_list:\n",
    "        all_values = record.split(',')\n",
    "        inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        targets = np.zeros(output_nodes) + 0.01\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets)\n",
    "        pass\n",
    "test_data_file = open(\"mldata/mnist_test_10.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "scorecard = []\n",
    "for record in test_data_list:\n",
    "    all_values = record.split(',')\n",
    "    correct_label = int(all_values[0])\n",
    "    print(correct_label, \"correct label\")\n",
    "    inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    outputs = n.query(inputs)\n",
    "    label = np.argmax(outputs)\n",
    "    print(label, \"network's answer\")\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    pass\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6e27f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total error rate is 0.05\n",
      "percentage of time spend playing video games?50\n",
      "frequent flier miles earned per year?1000\n",
      "liters of ice cream consumed per years?5\n",
      "You will probably like this person: in large doses\n",
      "7 correct label\n",
      "7 k-near's answer\n",
      "2 correct label\n",
      "2 k-near's answer\n",
      "1 correct label\n",
      "1 k-near's answer\n",
      "0 correct label\n",
      "0 k-near's answer\n",
      "4 correct label\n",
      "4 k-near's answer\n",
      "1 correct label\n",
      "1 k-near's answer\n",
      "4 correct label\n",
      "9 k-near's answer\n",
      "9 correct label\n",
      "4 k-near's answer\n",
      "5 correct label\n",
      "6 k-near's answer\n",
      "9 correct label\n",
      "9 k-near's answer\n",
      "performance =  0.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import operator as op\n",
    "def createDataSet():\n",
    "    group = np.array([[1.0,1.1], [1.0,1.0], [0,0], [0,0.1]])\n",
    "    labels = ['A', 'A', 'B', 'B']\n",
    "    return group, labels\n",
    "def classify0(inX, dataset, labels, k):\n",
    "    dataSetSize = dataset.shape[0]\n",
    "    diffMat = np.tile(inX, (dataSetSize, 1)) - dataset\n",
    "    sqdiffMat = diffMat ** 2\n",
    "    sqDistance = sqdiffMat.sum(axis=1)\n",
    "    distance = sqDistance ** 0.5\n",
    "    sortedDistIndicies = distance.argsort()\n",
    "    classCount = {}\n",
    "    for i in range(k):\n",
    "        voteIable = labels[sortedDistIndicies[i]]\n",
    "        classCount[voteIable] = classCount.get(voteIable, 0) + 1\n",
    "    sortedClassCount = sorted(classCount.items(), key=op.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "group, labels = createDataSet()\n",
    "classify0([0,0], group, labels, 3)\n",
    "def file2matrix(filename):\n",
    "    fr = open(filename)\n",
    "    arrayOLines = fr.readlines()\n",
    "    numberOfLines = len(arrayOLines)\n",
    "    returnMat = np.zeros((numberOfLines,3))\n",
    "    classLabelVector = []\n",
    "    index = 0\n",
    "    for line in arrayOLines:\n",
    "        line = line.strip()\n",
    "        listFromLine = line.split('\\t')\n",
    "        returnMat[index,:] = listFromLine[0:3]\n",
    "        classLabelVector.append(int(listFromLine[-1]))\n",
    "        index += 1\n",
    "    return returnMat, classLabelVector\n",
    "datingDataMat, datingLabels = file2matrix('mldata/datingTestSet2.txt')\n",
    "def autoNorm(dataSet):\n",
    "    minVals = dataSet.min(0)\n",
    "    maxVals = dataSet.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normDataSet = np.zeros(np.shape(dataSet))\n",
    "    m = dataSet.shape[0]\n",
    "    normDataSet = dataSet - np.tile(minVals, (m, 1))\n",
    "    normDataSet = normDataSet/np.tile(ranges, (m, 1))\n",
    "    return normDataSet, ranges, minVals\n",
    "normMat, ranges, minVals = autoNorm(datingDataMat)\n",
    "def datingClassTest():\n",
    "    hoRatio = 0.10\n",
    "    datingDataMat, datingLabels = file2matrix('mldata/datingTestSet2.txt')\n",
    "    normMat, ranges, minVals = autoNorm(datingDataMat)\n",
    "    m = normMat.shape[0]\n",
    "    numTestVecs = int(m*hoRatio)\n",
    "    errorCount = 0.0\n",
    "    for i in range(numTestVecs):\n",
    "        classifierResult = classify0(normMat[i,:], normMat[numTestVecs:m,:], datingLabels[numTestVecs:m], 3)\n",
    "        #print(f\"the classifier came back with: {classifierResult}, the real answer is {datingLabels[i]}\")\n",
    "        if classifierResult != datingLabels[i]:\n",
    "            errorCount += 1.0\n",
    "    print(f\"the total error rate is {errorCount/float(numTestVecs)}\")\n",
    "datingClassTest()\n",
    "def classifyPerson():\n",
    "    resultList = ['not at all', 'in small doses', 'in large doses']\n",
    "    percentTats = float(input(\"percentage of time spend playing video games?\"))\n",
    "    ffMiles = float(input(\"frequent flier miles earned per year?\"))\n",
    "    iceCream = float(input(\"liters of ice cream consumed per years?\"))\n",
    "    datingDataMat, datingLabels = file2matrix('mldata/datingTestSet2.txt')\n",
    "    normMat, ranges, minVals = autoNorm(datingDataMat)\n",
    "    inArr = np.array([ffMiles, percentTats, iceCream])\n",
    "    classifierResult = classify0((inArr - minVals)/ranges, normMat, datingLabels, 3)\n",
    "    print(f\"You will probably like this person: {resultList[classifierResult - 1]}\")\n",
    "classifyPerson()\n",
    "def handwritingClassTest():\n",
    "    hwLabels = []\n",
    "    training_data_file = open(\"mldata/mnist_train_100.csv\", 'r')\n",
    "    training_data_list = training_data_file.readlines()\n",
    "    training_data_file.close()\n",
    "    m = len(training_data_list)\n",
    "    trainingMat = np.zeros((m, 28 * 28))\n",
    "    hwLabels = []\n",
    "    for i in range(m):\n",
    "        all_values = training_data_list[i].split(',')\n",
    "        hwLabels.append(int(all_values[0]))\n",
    "        trainingMat[i,:] = all_values[1:]\n",
    "    test_data_file = open(\"mldata/mnist_test_10.csv\", 'r')\n",
    "    test_data_list = test_data_file.readlines()\n",
    "    test_data_file.close()\n",
    "    scorecard = []\n",
    "    for record in test_data_list:\n",
    "        all_values = record.split(',')\n",
    "        correct_label = int(all_values[0])\n",
    "        print(correct_label, \"correct label\")\n",
    "        inputs = np.asfarray(all_values[1:])\n",
    "        label = classify0(inputs, trainingMat, hwLabels, 3)\n",
    "        print(label, \"k-near's answer\")\n",
    "        if (label == correct_label):\n",
    "            scorecard.append(1)\n",
    "        else:\n",
    "            scorecard.append(0)\n",
    "    scorecard_array = np.asarray(scorecard)\n",
    "    print (\"performance = \", scorecard_array.sum() / scorecard_array.size)\n",
    "handwritingClassTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a977661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "[[1, 'no'], [1, 'no']]\n",
      "no\n",
      "yes\n",
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
      "hard\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import operator\n",
    "import pickle\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        labelCounts[currentLabel] = labelCounts[currentLabel] + 1 if currentLabel in labelCounts.keys() else 1\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        shannonEnt -= prob * math.log(prob, 2)\n",
    "    return shannonEnt\n",
    "def createDataSet():\n",
    "    dataSet = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet,labels\n",
    "myDat,labels = createDataSet()\n",
    "print(calcShannonEnt(myDat))\n",
    "#myDat[0][-1]='maybe'\n",
    "#print(calcShannonEnt(myDat))\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet=[]\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "print(splitDataSet(myDat,0,1))\n",
    "print(splitDataSet(myDat,0,0))\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        if infoGain > bestInfoGain:\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "chooseBestFeatureToSplit(myDat)\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sort(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "def createTree(dataSet, labels):\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    del(labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n",
    "    return myTree\n",
    "myTree = createTree(myDat, labels)\n",
    "myDat,labels = createDataSet()\n",
    "def classify(inputTree, featLabels, testVec):\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    secondDict = inputTree[firstStr]\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel\n",
    "print(classify(myTree, labels, [1,0]))\n",
    "print(classify(myTree, labels, [1,1]))\n",
    "def storeTree(inputTree, filename):\n",
    "    fw = open(filename, 'wb')\n",
    "    pickle.dump(inputTree, fw, 2)\n",
    "    fw.close()\n",
    "def grabTree(filename):\n",
    "    fr = open(filename, 'rb')\n",
    "    return pickle.load(fr)\n",
    "storeTree(myTree, 'mldata/tree.bin')\n",
    "print(grabTree('mldata/tree.bin'))\n",
    "fr = open('mldata/lenses.txt')\n",
    "lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n",
    "lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "lensesTree = createTree(lenses, lensesLabels)\n",
    "lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "print(classify(lensesTree, lensesLabels, ['young', 'hyper', 'yes', 'normal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "688dc551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'quit', 'I', 'stop', 'ate', 'garbage', 'to', 'so', 'licks', 'has', 'take', 'worthless', 'love', 'mr', 'steak', 'dalmation', 'please', 'cut', 'maybe', 'posting', 'buying', 'dog', 'him', 'problems', 'help', 'park', 'food', 'stupid', 'my', 'not', 'flea', 'how']\n",
      "[-3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -1.94591015\n",
      " -3.04452244 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -3.04452244\n",
      " -2.35137526 -2.35137526 -2.35137526 -1.94591015 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -1.65822808 -3.04452244 -2.35137526\n",
      " -3.04452244 -3.04452244]\n",
      "['love', 'my', 'dalmation'] classified as: 0\n",
      "['stupid', 'garbage'] classified as: 1\n",
      "'utf-8' codec can't decode byte 0x92 in position 884: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x92 in position 66: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xae in position 199: invalid start byte\n",
      "the error rate is: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                  ['my', 'dalmation', 'is', 'so', 'cut', 'I', 'love', 'him'],\n",
    "                  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                  ['mr','licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList,classVec\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in inputSet:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(f\"the word: {word} is not in my Vocabulary！\")\n",
    "    return returnVec\n",
    "def bagOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in inputSet:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "listOPosts,listClasses=loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "p0V,p1V,pAb=trainNB0(trainMat, listClasses)\n",
    "print(myVocabList)\n",
    "print(p1V)\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + math.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + math.log(1.0 - pClass1)\n",
    "    return 1 if p1 > p0 else 0\n",
    "def testingNB():\n",
    "    listOPosts,listClasses=loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb=trainNB0(trainMat, listClasses)\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(f'{testEntry} classified as: {classifyNB(thisDoc, p0V, p1V, pAb)}')\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(f'{testEntry} classified as: {classifyNB(thisDoc, p0V, p1V, pAb)}')\n",
    "testingNB()\n",
    "def textParse(bigString):\n",
    "    listOfTokens = re.split('\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "def spamTest():\n",
    "    docList=[]\n",
    "    classList=[]\n",
    "    fullText=[]\n",
    "    for i in range(1,26):\n",
    "        try:\n",
    "            wordList = textParse(open(f'mldata/email/spam/{i}.txt', encoding='utf-8').read())\n",
    "            docList.append(wordList)\n",
    "            fullText.append(wordList)\n",
    "            classList.append(1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            wordList = textParse(open(f'mldata/email/ham/{i}.txt', encoding='utf-8').read())\n",
    "            docList.append(wordList)\n",
    "            fullText.append(wordList)\n",
    "            classList.append(0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    vcabList = createVocabList(docList)\n",
    "    trainingSet = list(range(45))\n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[]\n",
    "    trainClasses=[]\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vcabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    with open('mldata/bayes.bin', 'wb') as f:\n",
    "        pickle.dump(p0V, f)\n",
    "        pickle.dump(p1V, f)\n",
    "        pickle.dump(pSpam, f)\n",
    "    with open('mldata/bayes.bin', 'rb') as f:\n",
    "        p0V = pickle.load(f)\n",
    "        p1V = pickle.load(f)\n",
    "        pSpam = pickle.load(f)\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vcabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print(f'the error rate is: {float(errorCount)/len(testSet)}')\n",
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "971403da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08108752]\n",
      " [-0.1233496 ]]\n",
      "[ 0.08344415 -0.17862418]\n",
      "[-0.05977823 -0.23760366]\n",
      "can't find logistic.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dangz\\AppData\\Local\\Temp\\ipykernel_9232\\3973054505.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1+np.exp(-inX))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "the error rate of this test is: 0.29850746268656714\n",
      "after 10 iterations the average error rate is: 0.29850746268656714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "def loadDataSet():\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open('mldata/testSet.txt')\n",
    "    for line in fr.readlines():\n",
    "        lineArr = line.strip().split()\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n",
    "        labelMat.append(int(lineArr[2]))\n",
    "    return dataMat,labelMat\n",
    "def sigmoid(inX):\n",
    "    return 1.0/(1+np.exp(-inX))\n",
    "def gradAscent(dataMatIn,classLabels):\n",
    "    dataMatrix = np.mat(dataMatIn)\n",
    "    labelMat = np.mat(classLabels).transpose()\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    alpha = 0.001\n",
    "    maxCycles = 500\n",
    "    weights = np.ones((n,1))\n",
    "    for k in range(maxCycles):\n",
    "        h = sigmoid(dataMatrix*weights)\n",
    "        error = labelMat - h\n",
    "        weights = weights + alpha * dataMatrix.transpose() * error\n",
    "    return weights\n",
    "dataArr,labelMat = loadDataSet()\n",
    "print(gradAscent(dataArr,labelMat))\n",
    "def stocGradAscent0(dataMatrix, classLabels, numIter=150):\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    alpha = 0.01\n",
    "    weights = np.ones(n)\n",
    "    for j in range(numIter):\n",
    "        for i in range(m):\n",
    "            h = sigmoid(sum(dataMatrix[i] * weights))\n",
    "            error = classLabels[i] - h\n",
    "            weights = weights + alpha * error * dataMatrix[i]\n",
    "    return weights\n",
    "print(stocGradAscent0(np.array(dataArr),labelMat))\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    weights = np.ones(n)\n",
    "    for j in range(numIter):\n",
    "        dataIndex = list(range(m))\n",
    "        for i in range(m):\n",
    "            alpha = 4/(1.0+j+i) + 0.01\n",
    "            randIndex = int(np.random.uniform(0,len(dataIndex)))\n",
    "            h = sigmoid(sum(dataMatrix[randIndex] * weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            del(dataIndex[randIndex])\n",
    "    return weights\n",
    "print(stocGradAscent1(np.array(dataArr),labelMat))\n",
    "def classifyVector(inX, weights):\n",
    "    prob = sigmoid(sum(inX*weights))\n",
    "    if prob > 0.5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "def colicTest():\n",
    "    frTrain=open('mldata/horseColicTraining.txt')\n",
    "    frTest=open('mldata/horseColicTest.txt')\n",
    "    trainingSet=[]\n",
    "    trainingLabels=[]\n",
    "    for line in frTrain.readlines():\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr=[]\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        trainingSet.append(lineArr)\n",
    "        trainingLabels.append(float(currLine[21]))\n",
    "    trainWeights = None\n",
    "    try:\n",
    "        with open('mldata/logistic.bin', 'rb') as f:\n",
    "            trainWeights = pickle.load(f)\n",
    "    except:\n",
    "        print(\"can't find logistic.bin\")\n",
    "    if type(trainWeights) != np.ndarray:\n",
    "        trainWeights = stocGradAscent1(np.array(trainingSet),trainingLabels,500)\n",
    "        with open('mldata/logistic.bin', 'wb') as f:\n",
    "            pickle.dump(trainWeights,f)\n",
    "    errorCount=0\n",
    "    numTestVec=0.0\n",
    "    for line in frTest.readlines():\n",
    "        numTestVec += 1.0\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr=[]\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        if int(classifyVector(np.array(lineArr),trainWeights)) != int(currLine[21]):\n",
    "            errorCount += 1\n",
    "    errorRate=float(errorCount)/numTestVec\n",
    "    print(f'the error rate of this test is: {errorRate}')\n",
    "    return errorRate\n",
    "def multiTest():\n",
    "    numTests=10\n",
    "    errorSum=0.0\n",
    "    for k in range(numTests):\n",
    "        errorSum += colicTest()\n",
    "    print(f'after {numTests} iterations the average error rate is: {errorSum/float(numTests)}')\n",
    "multiTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9727103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.98647356]\n",
      " [0.98647356 1.        ]]\n",
      "[[1.         0.99931945]\n",
      " [0.99931945 1.        ]]\n",
      "56.8118936813369\n",
      "429.89056187020685\n",
      "549.1181708824906\n",
      "94927.34165777494\n",
      "573.5261441896996\n",
      "517.5711905384079\n",
      "[[ 0.04304419 -0.02274163  0.13214088  0.02075182  2.22403745 -0.99895298\n",
      "  -0.11725424  0.16622922]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0.   0.   0.   0.01 0.   0.   0.   0.  ]]\n",
      "[[0.   0.   0.   0.02 0.   0.   0.   0.  ]]\n",
      "[[0.   0.   0.   0.03 0.   0.   0.   0.  ]]\n",
      "[[0.   0.   0.   0.04 0.   0.   0.   0.  ]]\n",
      "[[0.   0.   0.   0.05 0.   0.   0.   0.  ]]\n",
      "[[0.   0.   0.   0.06 0.   0.   0.   0.  ]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.  ]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.01]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.02]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.03]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.04]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.05]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.06]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.07]]\n",
      "[[0.   0.   0.01 0.06 0.   0.   0.   0.08]]\n",
      "[[0.   0.   0.01 0.05 0.   0.   0.   0.08]]\n",
      "[[0.   0.   0.01 0.05 0.   0.   0.   0.09]]\n",
      "[[0.   0.   0.01 0.05 0.   0.   0.   0.1 ]]\n",
      "[[0.   0.   0.01 0.05 0.   0.   0.   0.11]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.01  0.    0.11]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.02  0.    0.11]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.02  0.    0.12]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.03  0.    0.12]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.03  0.    0.13]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.04  0.    0.13]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.05  0.    0.13]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.05  0.    0.14]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.06  0.    0.14]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.07  0.    0.14]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.07  0.    0.15]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.08  0.    0.15]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.08  0.    0.16]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.09  0.    0.16]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.1   0.    0.16]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.1   0.    0.17]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.11  0.    0.17]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.12  0.    0.17]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.12  0.    0.18]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.13  0.    0.18]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.13  0.    0.19]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.14  0.    0.19]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.15  0.    0.19]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.15  0.    0.2 ]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.16  0.    0.2 ]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.16  0.    0.21]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.17  0.    0.21]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.18  0.    0.21]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.18  0.    0.22]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.19  0.    0.22]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.2   0.    0.22]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.2   0.    0.23]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.21  0.    0.23]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.21  0.    0.24]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.22  0.    0.24]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.23  0.    0.24]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.23  0.    0.25]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.24  0.    0.25]]\n",
      "[[ 0.    0.    0.01  0.05  0.   -0.25  0.    0.25]]\n",
      "[[ 0.    0.    0.02  0.05  0.   -0.25  0.    0.25]]\n",
      "[[ 0.    0.    0.02  0.04  0.   -0.25  0.    0.25]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.25  0.    0.25]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.25  0.    0.26]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.26  0.    0.26]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.26  0.    0.27]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.27  0.    0.27]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.28  0.    0.27]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.28  0.    0.28]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.29  0.    0.28]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.29  0.    0.29]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.3   0.    0.29]]\n",
      "[[ 0.    0.    0.03  0.04  0.   -0.31  0.    0.29]]\n",
      "[[ 0.    0.    0.04  0.04  0.   -0.31  0.    0.29]]\n",
      "[[ 0.    0.    0.04  0.04  0.   -0.32  0.    0.29]]\n",
      "[[ 0.    0.    0.04  0.04  0.   -0.33  0.    0.29]]\n",
      "[[ 0.    0.    0.04  0.04  0.   -0.33  0.    0.3 ]]\n",
      "[[ 0.    0.    0.04  0.04  0.   -0.34  0.    0.3 ]]\n",
      "[[ 0.    0.    0.04  0.04  0.   -0.35  0.    0.3 ]]\n",
      "[[ 0.    0.    0.05  0.04  0.   -0.35  0.    0.3 ]]\n",
      "[[ 0.    0.    0.05  0.04  0.   -0.36  0.    0.3 ]]\n",
      "[[ 0.    0.    0.05  0.04  0.   -0.37  0.    0.3 ]]\n",
      "[[ 0.    0.    0.05  0.04  0.   -0.37  0.    0.31]]\n",
      "[[ 0.    0.    0.05  0.04  0.   -0.38  0.    0.31]]\n",
      "[[ 0.    0.    0.05  0.04  0.   -0.39  0.    0.31]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.39  0.    0.31]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.4   0.    0.31]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.41  0.    0.31]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.41  0.    0.32]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.42  0.    0.32]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.42  0.    0.33]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.43  0.    0.33]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.44  0.    0.33]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.44  0.    0.34]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.45  0.    0.34]]\n",
      "[[ 0.    0.    0.06  0.04  0.   -0.46  0.    0.34]]\n",
      "[[ 0.    0.    0.07  0.04  0.   -0.46  0.    0.34]]\n",
      "[[ 0.    0.    0.07  0.03  0.   -0.46  0.    0.34]]\n",
      "[[ 0.    0.    0.08  0.03  0.   -0.46  0.    0.34]]\n",
      "[[ 0.    0.    0.08  0.03  0.   -0.46  0.    0.35]]\n",
      "[[ 0.    0.    0.08  0.03  0.   -0.47  0.    0.35]]\n",
      "[[ 0.    0.    0.08  0.03  0.   -0.47  0.    0.36]]\n",
      "[[ 0.    0.    0.08  0.03  0.   -0.48  0.    0.36]]\n",
      "[[ 0.    0.    0.08  0.03  0.   -0.49  0.    0.36]]\n",
      "[[ 0.    0.    0.09  0.03  0.   -0.49  0.    0.36]]\n",
      "[[ 0.    0.    0.09  0.03  0.   -0.5   0.    0.36]]\n",
      "[[ 0.    0.    0.09  0.03  0.   -0.51  0.    0.36]]\n",
      "[[ 0.    0.    0.09  0.03  0.   -0.51  0.    0.37]]\n",
      "[[ 0.    0.    0.09  0.03  0.   -0.52  0.    0.37]]\n",
      "[[ 0.    0.    0.09  0.03  0.01 -0.52  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.01 -0.52  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.01 -0.53  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.02 -0.53  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.03 -0.53  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.04 -0.53  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.04 -0.54  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.05 -0.54  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.06 -0.54  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.06 -0.55  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.07 -0.55  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.08 -0.55  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.08 -0.56  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.09 -0.56  0.    0.37]]\n",
      "[[ 0.01  0.    0.09  0.03  0.1  -0.56  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.1  -0.56  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.1  -0.57  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.11 -0.57  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.12 -0.57  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.13 -0.57  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.13 -0.58  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.14 -0.58  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.15 -0.58  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.15 -0.59  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.16 -0.59  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.17 -0.59  0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.17 -0.6   0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.18 -0.6   0.    0.37]]\n",
      "[[ 0.02  0.    0.09  0.03  0.19 -0.6   0.    0.37]]\n",
      "[[ 0.03  0.    0.09  0.03  0.19 -0.6   0.    0.37]]\n",
      "[[ 0.03  0.    0.09  0.03  0.19 -0.61  0.    0.37]]\n",
      "[[ 0.03  0.    0.09  0.03  0.2  -0.61  0.    0.37]]\n",
      "[[ 0.03  0.    0.09  0.03  0.21 -0.61  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.21 -0.61  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.22 -0.61  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.22 -0.62  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.23 -0.62  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.24 -0.62  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.24 -0.63  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.25 -0.63  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.26 -0.63  0.    0.37]]\n",
      "[[ 0.04  0.    0.09  0.03  0.26 -0.63  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.27 -0.63  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.28 -0.63  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.29 -0.63  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.29 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.3  -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]\n",
      "[[ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.05  0.    0.09 ... -0.64  0.    0.36]\n",
      " [ 0.04  0.    0.09 ... -0.64  0.    0.36]\n",
      " [ 0.05  0.    0.09 ... -0.64  0.    0.36]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def loadDataSet(filename):\n",
    "    numFeat = len(open(filename).readline().split('\\t')) - 1\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open(filename)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return dataMat,labelMat\n",
    "def standRegres(xArr, yArr):\n",
    "    xMat = np.mat(xArr)\n",
    "    yMat = np.mat(yArr).T\n",
    "    xTx = xMat.T*xMat\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat.T*yMat)\n",
    "    return ws\n",
    "xArr,yArr = loadDataSet('mldata/ex0.txt')\n",
    "ws = standRegres(xArr,yArr)\n",
    "yHat = np.mat(xArr) * ws\n",
    "print(np.corrcoef(yHat.T, np.mat(yArr)))\n",
    "def lwlr(testPoint,xArr,yArr,k=1.0):\n",
    "    xMat = np.mat(xArr)\n",
    "    yMat = np.mat(yArr).T\n",
    "    m = np.shape(xMat)[0]\n",
    "    weights = np.mat(np.eye(m))\n",
    "    for j in range(m):\n",
    "        diffMat = testPoint - xMat[j,:]\n",
    "        weights[j,j] = np.exp(diffMat * diffMat.T/(-2.0*k**2))\n",
    "    xTx = xMat.T * (weights * xMat)\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat.T*(weights*yMat))\n",
    "    return testPoint * ws\n",
    "def lwlrTest(testArr, xArr, yArr, k = 1.0):\n",
    "    m = np.shape(testArr)[0]\n",
    "    yHat = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        yHat[i] = lwlr(testArr[i], xArr, yArr, k)\n",
    "    return yHat\n",
    "print(np.corrcoef(lwlrTest(xArr, xArr, yArr, 0.003).T, np.mat(yArr)))\n",
    "def rssError(yArr, yHatArr):\n",
    "    return ((yArr - yHatArr)**2).sum()\n",
    "abX,abY=loadDataSet('mldata/abalone.txt')\n",
    "print(rssError(abY[0:99],lwlrTest(abX[0:99],abX[0:99],abY[0:99],0.1)))\n",
    "print(rssError(abY[0:99],lwlrTest(abX[0:99],abX[0:99],abY[0:99],1)))\n",
    "print(rssError(abY[0:99],lwlrTest(abX[0:99],abX[0:99],abY[0:99],10)))\n",
    "print(rssError(abY[100:199],lwlrTest(abX[100:199],abX[0:99],abY[0:99],0.1)))\n",
    "print(rssError(abY[100:199],lwlrTest(abX[100:199],abX[0:99],abY[0:99],1)))\n",
    "print(rssError(abY[100:199],lwlrTest(abX[100:199],abX[0:99],abY[0:99],10)))\n",
    "def ridgeRegres(xMat, yMat, lam=0.2):\n",
    "    xTx = xMat.T*xMat\n",
    "    denom = xTx + np.eye(np.shape(xMat)[1]) * lam\n",
    "    if np.linalg.det(denom) == 0.0:\n",
    "        print(\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = denom.I * (xMat.T * yMat)\n",
    "    return ws\n",
    "def ridgeTest(xArr, yArr):\n",
    "    xMat = np.mat(xArr)\n",
    "    yMat = np.mat(yArr).T\n",
    "    yMean = np.mean(yMat, 0)\n",
    "    yMat = yMat - yMean\n",
    "    xMeans = np.mean(xMat, 0)\n",
    "    xVar = np.var(xMat, 0)\n",
    "    xMat = (xMat - xMeans)/xVar\n",
    "    numTestPts = 1\n",
    "    wMat = np.zeros((numTestPts, np.shape(xMat)[1]))\n",
    "    for i in range(numTestPts):\n",
    "        ws = ridgeRegres(xMat, yMat, np.exp(i-10))\n",
    "        wMat[i,:] = ws.T\n",
    "    return wMat\n",
    "print(ridgeTest(abX, abY))\n",
    "def stageWise(xArr, yArr, eps=0.01, numIt=100):\n",
    "    xMat = np.mat(xArr)\n",
    "    yMat = np.mat(yArr).T\n",
    "    yMean = np.mean(yMat, 0)\n",
    "    yMat = yMat - yMean\n",
    "    xMeans = np.mean(xMat, 0)\n",
    "    xVar = np.var(xMat, 0)\n",
    "    xMat = (xMat - xMeans)/xVar\n",
    "    m,n=np.shape(xMat)\n",
    "    returnMat = np.zeros((numIt,n))\n",
    "    ws = np.zeros((n,1))\n",
    "    wsTest = ws.copy()\n",
    "    wsMax = ws.copy()\n",
    "    for i in range(numIt):\n",
    "        print(ws.T)\n",
    "        lowestError = np.inf\n",
    "        for j in range(n):\n",
    "            for sign in [-1,1]:\n",
    "                wsTest = ws.copy()\n",
    "                wsTest[j] += eps * sign\n",
    "                yTest = xMat * wsTest\n",
    "                rssE = rssError(yMat.A, yTest.A)\n",
    "                if rssE < lowestError:\n",
    "                    lowestError = rssE\n",
    "                    wsMax = wsTest\n",
    "        ws = wsMax.copy()\n",
    "        returnMat[i,:]=ws.T\n",
    "    return returnMat\n",
    "print(stageWise(abX, abY, 0.01, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1a0b6b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.379713\n",
      "-4.232586\n",
      "4.838138\n",
      "5.1904\n",
      "[[-0.4420751   1.94059639]\n",
      " [-0.16448688 -0.01128562]]\n",
      "5.184632816681332\n",
      "(60, 2)\n",
      "sseSplit: 144.82021724250035, sseNotSplit: 0.0\n",
      "bestCentToSplit: 0, len(bestClustAss): 60\n",
      "[[-0.006756050000000002, 3.2271029749999998], [-0.45965614999999993, -2.7782156000000002]]\n",
      "(40, 2)\n",
      "sseSplit: 49.61872046270045, sseNotSplit: 21.798183738423\n",
      "(20, 2)\n",
      "sseSplit: 14.698143155806289, sseNotSplit: 123.02203350407736\n",
      "bestCentToSplit: 0, len(bestClustAss): 40\n",
      "[[-2.94737575, 3.3263781000000003], [-0.45965614999999993, -2.7782156000000002], [2.93386365, 3.12782785]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(matrix([[-2.94737575,  3.3263781 ],\n",
       "         [-0.45965615, -2.7782156 ],\n",
       "         [ 2.93386365,  3.12782785]]),\n",
       " matrix([[2.        , 0.38139356],\n",
       "         [0.        , 0.82475077],\n",
       "         [1.        , 1.0108639 ],\n",
       "         [2.        , 1.15995155],\n",
       "         [0.        , 1.16351392],\n",
       "         [1.        , 1.96765728],\n",
       "         [2.        , 0.9150191 ],\n",
       "         [0.        , 0.46916551],\n",
       "         [1.        , 1.88098128],\n",
       "         [2.        , 2.72778511],\n",
       "         [0.        , 2.29797746],\n",
       "         [1.        , 0.16021061],\n",
       "         [2.        , 1.05804787],\n",
       "         [0.        , 0.4097449 ],\n",
       "         [1.        , 1.45510908],\n",
       "         [2.        , 1.22325471],\n",
       "         [0.        , 2.22177461],\n",
       "         [1.        , 0.09883066],\n",
       "         [2.        , 0.36394209],\n",
       "         [0.        , 0.79959117],\n",
       "         [1.        , 0.97045965],\n",
       "         [2.        , 1.31318515],\n",
       "         [0.        , 0.86641953],\n",
       "         [1.        , 0.38572737],\n",
       "         [2.        , 1.73329036],\n",
       "         [0.        , 2.27032493],\n",
       "         [1.        , 1.34281955],\n",
       "         [2.        , 1.65778703],\n",
       "         [0.        , 0.68327177],\n",
       "         [1.        , 1.13493488],\n",
       "         [2.        , 1.32967799],\n",
       "         [0.        , 1.88149507],\n",
       "         [1.        , 1.45779542],\n",
       "         [2.        , 1.07150386],\n",
       "         [0.        , 1.33509505],\n",
       "         [1.        , 0.93778763],\n",
       "         [2.        , 1.79809753],\n",
       "         [0.        , 0.8625163 ],\n",
       "         [1.        , 1.53712924],\n",
       "         [2.        , 0.50928442],\n",
       "         [0.        , 1.34913297],\n",
       "         [1.        , 1.45120312],\n",
       "         [2.        , 1.71629718],\n",
       "         [0.        , 1.58098963],\n",
       "         [1.        , 1.24481834],\n",
       "         [2.        , 0.97219835],\n",
       "         [0.        , 1.7087039 ],\n",
       "         [1.        , 1.06701049],\n",
       "         [2.        , 2.25715853],\n",
       "         [0.        , 1.28441083],\n",
       "         [1.        , 0.44602125],\n",
       "         [2.        , 1.2259755 ],\n",
       "         [0.        , 0.46190882],\n",
       "         [1.        , 1.46971227],\n",
       "         [2.        , 1.62315401],\n",
       "         [0.        , 0.27584383],\n",
       "         [1.        , 0.51009635],\n",
       "         [2.        , 0.05526451],\n",
       "         [0.        , 1.7798211 ],\n",
       "         [1.        , 1.26901537]]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def loadDataSet(filename):\n",
    "    dataMat = []\n",
    "    fr = open(filename)\n",
    "    for line in fr.readlines():\n",
    "        curLine = line.strip().split('\\t')\n",
    "        fltLine = list(map(float,curLine))\n",
    "        dataMat.append(fltLine)\n",
    "    return dataMat\n",
    "def distEclud(vecA, vecB):\n",
    "    #print(\"dist\", math.sqrt(np.sum(np.power(vecA - vecB, 2))))\n",
    "    return math.sqrt(np.sum(np.power(vecA - vecB, 2)))\n",
    "def randCent(dataSet, k):\n",
    "    n = np.shape(dataSet)[1]\n",
    "    centroids = np.mat(np.zeros((k,n)))\n",
    "    for j in range(n):\n",
    "        minJ = np.min(dataSet[:,j])\n",
    "        rangeJ = float(np.max(dataSet[:,j]) - minJ)\n",
    "        centroids[:,j] = minJ + rangeJ * np.random.rand(k,1)\n",
    "    return centroids\n",
    "datMat = np.mat(loadDataSet('mldata/unsupervised/testSet.txt'))\n",
    "print(np.min(datMat[:,0]))\n",
    "print(np.min(datMat[:,1]))\n",
    "print(np.max(datMat[:,0]))\n",
    "print(np.max(datMat[:,1]))\n",
    "print(randCent(datMat, 2))\n",
    "print(distEclud(datMat[0], datMat[1]))\n",
    "def kMeans(dataSet, k, distMeas = distEclud, createCent = randCent):\n",
    "    m = np.shape(dataSet)[0]\n",
    "    clusterAssment = np.mat(np.zeros((m, 2)))\n",
    "    centroids = createCent(dataSet, k)\n",
    "    clusterChanged = True\n",
    "    while clusterChanged:\n",
    "        clusterChanged = False\n",
    "        for i in range(m):\n",
    "            minDist = np.inf\n",
    "            minIndex = -1\n",
    "            for j in range(k):\n",
    "                distJI = distMeas(centroids[j,:], dataSet[i,:])\n",
    "                if distJI < minDist:\n",
    "                    minDist = distJI\n",
    "                    minIndex = j\n",
    "            if clusterAssment[i,0] != minIndex:\n",
    "                clusterChanged = True\n",
    "            clusterAssment[i,:] = minIndex, minDist\n",
    "        #print(centroids)\n",
    "        for cent in range(k):\n",
    "            pstInClust = dataSet[np.nonzero(clusterAssment[:,0].A == cent)[0]]\n",
    "            centroids[cent,:] = np.mean(pstInClust, axis=0)\n",
    "    return centroids, clusterAssment\n",
    "cent, cluster = kMeans(datMat, 4)\n",
    "def biKmeans(dataSet, k, distMeas = distEclud):\n",
    "    m = np.shape(dataSet)[0]\n",
    "    clusterAssment = np.mat(np.zeros((m,2)))\n",
    "    centroid0 = np.mean(dataSet, axis=0).tolist()[0]\n",
    "    centList = [centroid0]\n",
    "    for j in range(m):\n",
    "        clusterAssment[j,1] = distMeas(np.mat(centroid0), dataSet[j,:])\n",
    "    while len(centList) < k:\n",
    "        lowestSSE = np.inf\n",
    "        for i in range(len(centList)):\n",
    "            ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:,0].A==i)[0],:]\n",
    "            centroidMat, splitClusterAss = kMeans(ptsInCurrCluster, 2, distMeas)\n",
    "            print(splitClusterAss.shape)\n",
    "            sseSplit = np.sum(splitClusterAss[:,1])\n",
    "            sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:,0].A!=i)[0], 1])\n",
    "            print(f'sseSplit: {sseSplit}, sseNotSplit: {sseNotSplit}')\n",
    "            if sseSplit + sseNotSplit < lowestSSE:\n",
    "                bestCentToSplit = i\n",
    "                bestNewCents = centroidMat\n",
    "                bestClustAss = splitClusterAss.copy()\n",
    "                lowestSSE = sseSplit + sseNotSplit\n",
    "        bestClustAss[np.nonzero(bestClustAss[:,0].A==1)[0], 0] = len(centList)\n",
    "        bestClustAss[np.nonzero(bestClustAss[:,0].A==0)[0], 0] = bestCentToSplit\n",
    "        print(f'bestCentToSplit: {bestCentToSplit}, len(bestClustAss): {len(bestClustAss)}')\n",
    "        centList[bestCentToSplit] = bestNewCents[0,:].A.flatten().tolist()\n",
    "        centList.append(bestNewCents[1,:].A.flatten().tolist())\n",
    "        print(centList)\n",
    "        clusterAssment[np.nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:] = bestClustAss\n",
    "    return np.mat(centList), clusterAssment\n",
    "datMat3 = np.mat(loadDataSet('mldata/unsupervised/testSet2.txt'))\n",
    "biKmeans(datMat3, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
